\section{Introduction}
\label{sec:introduction}
% Copied from project description.
% TODO: Adapt. E.g. remove mention parallel analysis of functions and rather explain threads (create nodes are used for that)
Static analyzers examine the source code of a program to find semantic properties without having to execute it. To gain information about a program, many analyzers compute abstract values for the different program points that over-approximate the set of possible concrete states. This is done by generating a system of constraints, where the unknowns are program points, possibly together with some calling context. The instructions in the code give rise to constraints describing how they affect the abstract state. When the system of constraints is generated, the analyzer solves it for a solution through fix-point iteration. This means that from an initial non-satisfying assignment of values to unknowns, these values are updated according to the constraints until a fix-point is reached, and all constraints are satisfied. Well-known algorithms for fix-point iteration are round-robin iteration and the work-list algorithm. Round-robin algorithms can, however, only be used for finite constraint systems and the work-list algorithm usually requires static dependencies. In contrast, the top-down solver recursively explores a possibly infinite constraint system on demand and tracks dynamic dependencies on the fly. It aims to find solutions for a defined set of interesting unknowns, e.g., an end node of the main program. For that, it recursively evaluates stable values for unknowns that are needed to compute the values for the set of interesting ones. If the value for a stable unknown is needed, it can just be looked up and does not have to be calculated again. However, if a new value for an unknown is computed and updated during the solving process, it is necessary to destabilize all unknowns that depend on this updated unknown. This means that the values for the now destabilized unknowns have to be evaluated again as they have to factor in the new value of the updated unknown. These dependencies of unknowns are detected dynamically during the solving process and are saved in a data structure. Further data structures are used to keep track of stable unknowns and their values. 
The time of analysis grows together with program size. For example, an analysis of SQL lite 3 takes multiple hours with the Goblint static analyzer. A significant part of the overall analysis time is spent by the solver on finding a solution for the constraint system. Thus, improving the performance of a solver with respect to computation time seems a promising way to improve the speed of the whole analysis. An idea to approach this issue is to equip the top-down solver with a way to execute tasks in parallel and find steps of the solving process, where a speedup can likely be achieved through parallelization. For example, an unknown can depend on multiple other unknowns, e.g., when at a given point in the program, it is not known which of two functions is executed and thus both functions have to be analyzed. In this case, one can imagine, that the recursive computation of a stable value for the unknowns corresponding to the different functions could be parallelized, and the computation time could be reduced.
In our research, we want to investigate existing approaches in related work and gain potential insights into the parallelization of solvers for constraint systems.
As our main contribution, we aim to implement a parallelized top-down solver for the Goblint analyzer.
% TODO: Add chapter references
For this, the first step is to protect the existing data structures used by the solver to make them thread-safe and thus avoid race conditions. We allow the right-hand sides of the constraints to indicate which unknowns will certainly be queried in the evaluation, as parallelization is only reasonable for those. Furthermore, we want to determine locations in the program where parallelization of the solver can be utilized and implement it for some of those instances. The solver is extended with a thread pool to allow multiple instances of it to work in parallel. We also have to identify critical sections of the solver, where synchronization is necessary, as well as establish communication between the threads, for example when unknowns have to be destabilized. This has to happen when the value of an unknown is no longer stable and might change in future iterations.
Lastly, we evaluate our implementation using the SV-Comp benchmark suite. As metrics, we use the number of timeouts and precise verdicts, as we hope to avoid some timeouts and thus find more correct solutions through the implemented parallelization. Furthermore, we also evaluate the efficiency by considering the analysis time per program.
