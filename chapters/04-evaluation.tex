\section{Evaluation}
\label{sec:eval}
We want to evaluate and compare our implementations of the three different parallel solvers in \gob. Before comparing their performance with respect to evaluation time, we investigate if the parallelization results in a loss of precision.
The source repository of \gob\ contains more than 1400 regression tests. These are small programs focussed on testing various edge cases of the different features of the analyzer. We executed all regression tests five times with each of the parallelized solvers from~\autoref{sec:method:td_parallel}. From the results, we observe that for less than 0.5\% of the tests the parallelized solvers were less precise than the single-threaded solver we introduced in~\autoref{sec:background:td}. This loss of precision was non-deterministic, i.e., in some runs there was no loss of precision observed. We do not notice a difference in the number of failing tests for the three different parallelized solvers. Furthermore, when analyzing real-world programs like we do in the following sections, no loss of precision was observed in the warnings \gob\ produced. These warnings include but are not limited to notifications about thread-(un)save memory locations and dead lines of code. Thus, we believe that the parallelized solvers can be considered as precise as the single-threaded solver we compared them against. We note here that \gob\ typically uses an improved version of the single-threaded \ac{td} from this report and thus is even more precise in 32 of the regression tests.

  \subsection{Analysis speed}
  \label{sec:eval:speed}
  To evaluate the speed of the different solvers, we analyze several programs and compare the time of the analysis. We use two groups of programs for this evaluation. The first are a selection of the GNU core utility programs (coreutils)~\cite{gnuCoreutils}. Before analysis, these programs were combined, i.e., a single source code file was produced, that contains the original program together with the dependencies of other included files. The exact code files used can be found in the \gob\ benchmark repository~\cite{goblintBench} that contains benchmark programs for the analyzed.
  The second group are programs from the pthread folder of the \gob\ benchmark repository. These are programs that use threads more extensively than the coreutil programs, i.e., they use threads slightly more often and the spawned threads perform much more work. We included these programs, since the shared memory \ac{td} and the disjunct \ac{td} depend on threads being spawned in the analyzed program to add tasks that are solved in parallel.\\
  For comparison, we analyze each program once with the single-threaded \ac{td} from~\autoref{sec:background:td} as a baseline and twice with each of the parallelized \acp{td} from~\autoref{sec:method:td_parallel}, where one run was done with only the main thread working and one run with two worker threads. We do this, so we can compare the analysis time between the single-threaded \ac{td} and a certain parallelized \ac{td} in more detail, e.g., attribute differences in analysis time to the inherently different algorithms of the solvers or to the parallelization. Since the programs are different in size and general analysis time, we mainly focus on the \textit{speedup} as a metric. We calculate the speedup of the parallelized solvers with the following formula:
  \begin{equation*}
    \text{speedup(parallel solver)} = \frac{\text{time(baseline solver)}}{\text{time(parallel solver)}}
  \end{equation*}
  With this we get a value larger than 1, if the parallelized solver is faster and a value smaller than 1 if it is slower. The raw timing values are listed in the appendix in~\autoref{fig:timing_all}.
  \begin{figure}
    %TODO: Adapt Colors. !!!WATCH CAPTION!!!
    \centering
    \begin{tikzpicture}
      \begin{axis}[ymin=-0.6, ymax=1.2, xmin=0, xmax=1, yticklabel=\empty, xticklabel=\empty, x tick style=transparent, y tick style=transparent]
          \draw (axis cs:0,-0.4) -- (axis cs:1,-0.4);
          \draw (axis cs:0,-0.2) -- (axis cs:1,-0.2);
          \draw (axis cs:0,0) -- (axis cs:1,0);
          \draw (axis cs:0,0.2) -- (axis cs:1,0.2);
          \draw (axis cs:0,0.4) -- (axis cs:1,0.4);
          \draw (axis cs:0,0.6) -- (axis cs:1,0.6);
          \draw (axis cs:0,0.8) -- (axis cs:1,0.8);
          \draw (axis cs:0,1) -- (axis cs:1,1);
      \end{axis}
      \begin{axis}[
          ybar=0pt,
          ymin=-0.6,ymax=1.2,
          ylabel = speedup,
          ytick pos = left,
          ytick distance = 0.2,
          ytick style = transparent,
          yticklabels={,0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2},
          symbolic x coords={cksum, cp, cut, dd,df,du,ls,mv,nohup,ptx,sort,tail,,aget,knot,pfscan,ctrace,smtprc,zebedee},
          xtick=data,
          xtick style = transparent,
          x tick label style={rotate=90, xshift=5pt},
          enlarge x limits={abs=6pt},
          bar width = 4pt,
          legend style={at={(0.35,0.97)},anchor=north east, font=\footnotesize, legend cell align=left,},
          ]
          \addplot[ybar, fill=blue, area legend] coordinates {
            (cksum,0.1)
            (cp,-0.25)
            (cut,-0.19)
            (dd,-0.10)
            (df,-0.08)
            (du,-0.12)
            (ls,-0.09)
            (mv,-0.13)
            (nohup,-0.11)
            (ptx,-0.16)
            (sort,-0.09)
            (tail,-0.19)
            (aget,-0.07)
            (knot,-0.12)
            (pfscan,-0.10)
            (ctrace,0.05)
            (smtprc,-0.11)
            (zebedee,-0.15)
            }; 
          \addplot[ybar, fill=red, area legend] coordinates {
            (cksum,0.08)
            (cp,-0.34)
            (cut,-0.15)
            (dd,-0.11)
            (df,0.22)
            (du,-0.01)
            (ls,0.30)
            (mv,-0.04)
            (nohup,-0.12)
            (ptx,-0.17)
            (sort,-0.04)
            (tail,-0.10)
            (aget,0.02)
            (knot,-0.6)
            (pfscan,0.02)
            (ctrace,0.01)
            (smtprc,-0.10)
            (zebedee,-0.10)
            }; 
      \legend{1 Thread, 2 Threads}  
      \end{axis}
    \end{tikzpicture}
    \caption[Speedup of the stealing \ac{td}]{Speedup of the stealing \ac{td}. Runs with a single worker thread are shown in blue, while runs with two threads are represented by red bars. The twelve leftmost programs make up the coreutil group, while the six rightmost programs belong to the pthread group of programs. The run with two threads for the \texttt{knot} program resulted in a timeout of 900s}
    \label{fig:speedup_stealing}
  \end{figure}

  When investigating the results for the stealing \ac{td} from~\autoref{fig:speedup_stealing}, we notice, that in general this solver is slower or just as fast as the baseline. Furthermore, the difference between running this solver single threaded or with two worker threads is minimal. Interesting are the programs \texttt{df} and \texttt{ls} where the stealing \ac{td} achieved a speedup of 1.2 and 1.3 respectively. However, for the program \texttt{cp} it was significantly slower than the baseline. Unfortunately we are not able to explain the timeout for the \texttt{knot} program, as it terminated in a reasonable time when analyzed with a debugging configuration. We note here, that we only evaluate the stealing \ac{td} with the addition of revival, since the stealing \ac{td} is strictly faster with revival.
  \begin{figure}
    %TODO: Adapt Colors. !!!WATCH CAPTION!!!
    \centering
    \begin{tikzpicture}
      \begin{axis}[ymin=-0.6, ymax=1.2, xmin=0, xmax=1, yticklabel=\empty, xticklabel=\empty, x tick style=transparent, y tick style=transparent]
          \draw (axis cs:0,-0.4) -- (axis cs:1,-0.4);
          \draw (axis cs:0,-0.2) -- (axis cs:1,-0.2);
          \draw (axis cs:0,0) -- (axis cs:1,0);
          \draw (axis cs:0,0.2) -- (axis cs:1,0.2);
          \draw (axis cs:0,0.4) -- (axis cs:1,0.4);
          \draw (axis cs:0,0.6) -- (axis cs:1,0.6);
          \draw (axis cs:0,0.8) -- (axis cs:1,0.8);
          \draw (axis cs:0,1) -- (axis cs:1,1);
      \end{axis}
      \begin{axis}[
          ybar=0pt,
          ymin=-0.6,ymax=1.2,
          ylabel = speedup,
          ytick pos = left,
          ytick distance = 0.2,
          ytick style = transparent,
          yticklabels={,0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2},
          symbolic x coords={cksum, cp, cut, dd,df,du,ls,mv,nohup,ptx,sort,tail,,aget,knot,pfscan,ctrace,smtprc,zebedee},
          xtick=data,
          xtick style = transparent,
          x tick label style={rotate=90, xshift=5pt},
          enlarge x limits={abs=6pt},
          bar width = 4pt,
          legend style={at={(0.35,0.97)},anchor=north east, font=\footnotesize, legend cell align=left,},
          ]
          \addplot[ybar, fill=blue, area legend] coordinates {
            (cksum,-0.31)
            (cp,-0.46)
            (cut,-0.59)
            (dd,-0.31)
            (df,-0.16)
            (du,-0.34)
            (ls,-0.17)
            (mv,-0.26)
            (nohup,-0.55)
            (ptx,-0.45)
            (sort,-0.27)
            (tail,-0.41)
            (aget,-0.20)
            (knot,-0.36)
            (pfscan,-0.03)
            (ctrace,-0.20)
            (smtprc,-0.05)
            (zebedee,-0.32)
            }; 
          \addplot[ybar, fill=red, area legend] coordinates {
            (cksum,-0.22)
            (cp,-0.52)
            (cut,-0.48)
            (dd,-0.32)
            (df,-0.26)
            (du,-0.42)
            (ls,-0.30)
            (mv,-0.34)
            (nohup,-0.25)
            (ptx,-0.32)
            (sort,-0.33)
            (tail,-0.47)
            (aget,0.16)
            (knot,0.03)
            (pfscan,0.27)
            (ctrace,0.40)
            (smtprc,0.44)
            (zebedee,0.03)
            }; 
      \legend{1 Thread, 2 Threads}  
      \end{axis}
    \end{tikzpicture}
    \caption[Speedup of the shared memory \ac{td}]{Speedup of the shared memory \ac{td}. Runs with a single worker thread are shown in blue, while runs with two threads are represented by red bars. The twelve leftmost programs make up the coreutil group, while the six rightmost programs belong to the pthread group of programs}
    \label{fig:speedup_shared_mem}
  \end{figure}

  \autoref{fig:speedup_shared_mem} shows the speedup results for the shared memory \ac{td}. We see this solver being significantly slower for all the coreutil programs, with the double threaded configuration only outperforming the single threaded one in three cases. However, the pthread group of programs paint a different picture. As expected, the single threaded configuration is slower than the baseline for these programs. When allowing the shared memory \ac{td} to use a second worker thread for parallel work, it achieves significant speedup for four of the six programs compared to the baseline. This speedup even reaches 1.4 for two of these.
  \begin{figure}
        %TODO: Adapt Colors. !!!WATCH CAPTION!!!
        \centering
        \begin{tikzpicture}
          \begin{axis}[ymin=-0.6, ymax=1.2, xmin=0, xmax=1, yticklabel=\empty, xticklabel=\empty, x tick style=transparent, y tick style=transparent]
              \draw (axis cs:0,-0.4) -- (axis cs:1,-0.4);
              \draw (axis cs:0,-0.2) -- (axis cs:1,-0.2);
              \draw (axis cs:0,0) -- (axis cs:1,0);
              \draw (axis cs:0,0.2) -- (axis cs:1,0.2);
              \draw (axis cs:0,0.4) -- (axis cs:1,0.4);
              \draw (axis cs:0,0.6) -- (axis cs:1,0.6);
              \draw (axis cs:0,0.8) -- (axis cs:1,0.8);
              \draw (axis cs:0,1) -- (axis cs:1,1);
          \end{axis}
          \begin{axis}[
              ybar=0pt,
              ymin=-0.6,ymax=1.2,
              ylabel = speedup,
              ytick pos = left,
              ytick distance = 0.2,
              ytick style = transparent,
              yticklabels={,0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2},
              symbolic x coords={cksum, cp, cut, dd,df,du,ls,mv,nohup,ptx,sort,tail,,aget,knot,pfscan,ctrace,smtprc,zebedee},
              xtick=data,
              xtick style = transparent,
              x tick label style={rotate=90, xshift=5pt},
              enlarge x limits={abs=6pt},
              bar width = 4pt,
              legend style={at={(0.35,0.97)},anchor=north east, font=\footnotesize, legend cell align=left,},
              ]
              \addplot[ybar, fill=blue, area legend] coordinates {
                (cksum,-0.06)
                (cp,-0.31)
                (cut,-0.28)
                (dd,-0.07)
                (df,0.02)
                (du,0.02)
                (ls,0.01)
                (mv,0.02)
                (nohup,-0.28)
                (ptx,-0.05)
                (sort,-0.11)
                (tail,0.00)
                (aget,0.00)
                (knot,0.00)
                (pfscan,0.42)
                (ctrace,0.05)
                (smtprc,0.45)
                (zebedee,-0.01)
                }; 
              \addplot[ybar, fill=red, area legend] coordinates {
                (cksum,0.13)
                (cp,-0.32)
                (cut,-0.09)
                (dd,-0.06)
                (df,-0.09)
                (du,-0.09)
                (ls,-0.13)
                (mv,-0.06)
                (nohup,0.00)
                (ptx,-0.01)
                (sort,-0.13)
                (tail,-0.08)
                (aget,0.46)
                (knot,0.59)
                (pfscan,0.77)
                (ctrace,0.91)
                (smtprc,1.11)
                (zebedee,0.83)
                }; 
          \legend{1 Thread, 2 Threads}  
          \end{axis}
        \end{tikzpicture}
    \caption[Speedup of the disjunct \ac{td} with sharing of side-effected variables]{Speedup of the disjunct \ac{td} with sharing of side-effected variables. Runs with a single worker thread are shown in blue, while runs with two threads are represented by red bars. The twelve leftmost programs make up the coreutil group, while the six rightmost programs belong to the pthread group of programs}
    \label{fig:speedup_disjunct}
  \end{figure}

  Through our benchmarks we noticed that the variant of the disjunct \ac{td} where side-effected variables are shared is slower when analyzing the coreutil programs but faster for the pthread programs compared to the original disjunct \ac{td}. The original variant is approximately as fast as the baseline for the coreutil programs, but achieves a speedup of around 1.8 for three of the pthread programs.
  Like the previously discussed shared memory \ac{td}, the disjunct \ac{td} is built around the \texttt{create} edges we inserted into the constraint system. Therefore, we are most interested in the performance on the pthread programs. In the following we focus on the speedups for the variant that shares side-effected variables, since it performed better for the pthread programs. The speedups for the different programs are illustrated in~\autoref{fig:speedup_disjunct}. As mentioned, for the coreutil programs we see the disjunct \ac{td} being slower than or about as fast as the baseline. For the pthread group of programs however, this solver provides significant speedup when run with two worker threads. Interestingly enough, the speedup for the \texttt{smtprc} program exceedsd 2.0, which is in theory the highest possible speedup that can be achieved by doubling the number of worker threads. We believe the reason for this peculiarity is the fact, that the solver data is distributed over multiple tasks, reducing the size of the individual data-structures. The frequent accesses to smaller data-structures are likely quicker. Thus, it is plausible that the disjunct \ac{td} can achieve a speedup higher than 2.0 when having two threads available. The solver data being distributed in smaller data-structures would also explain, how the disjunct \ac{td} can be faster than the baseline even when it is just run with a single worker thread like it is the case for the \texttt{pfscan} and the \texttt{smtprc} program.\\
  To investigate if there is a correlation between program size and speedup, we computed the Pearson correlation coefficient of the speedup a solver achieves with two worker threads and the number of live lines of code in the program. These are the lines that correspond to program points which are reached during the solving process. For the stealing \ac{td} the correlation coefficient amounts to 0.03. For this computation we included all programs except for \texttt{knot}, where the solver timeouted. For the other two solvers we only included the pthread group of programs, since these are the programs, where the \texttt{create} edges are suitably placed for these solvers. The correlation coefficient is 0.04 for the shared memory \ac{td} and 0.12 for the disjunct \ac{td}. This means, there is little to no correlation between program size and speedup achieved.
  Interestingly enough, when we compute the correlation coefficient with respect to the coreutil programs, we get coefficients of -0.29 for the shared memory \ac{td} and -0.68 for the disjunct td. This means, that there is a small but noticeable negative correlation for the programs, that are not suitable to be analyzed with these solvers. This indicates that these solvers scale worse than the baseline for larger programs that make little or no use of threads.\\
  Overall, we see the disjunct \ac{td} achieving a significant speedup on the pthread group programs. While the shared memory \ac{td} also provides a speedup for these programs, the disjunct \ac{td} outperforms it in almost every case. The stealing \ac{td}, implementing a very different approach that does not depend on \texttt{create} edges in the constraint system, does not provide a speedup for the pthread programs, but it is the only one of the three solvers that achieves a noticeable speedup for any of the coreutil programs, i.e., the \texttt{df} and the \texttt{ls} program.
